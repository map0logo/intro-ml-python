---
title: Python para Aprendizaje Autom√°tico 3
subtitle: M√©todos No Supervisados
date: last-modified
author:
  - name: Francisco Palm
    orcid: 0000-0002-1293-0868
    email: fpalm@qu4nt.com
    affiliations: qu4nt, activistasxsl
format:
  clean-revealjs
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
lang: es
logo: images/qu4nt-logo.png
---

# M√©todos No Supervisados

## Clustering y Reducci√≥n de Dimensionalidad

---

## Agenda

- **Parte I: Introducci√≥n al Aprendizaje No Supervisado**
- **Parte II: Clustering con K-means**
- **Parte III: Clustering Espectral**
- **Parte IV: Reducci√≥n de Dimensionalidad**
- **Parte V: Discusi√≥n y Casos Pr√°cticos**

---

## ¬øQu√© es el Aprendizaje No Supervisado?

**Aprendizaje Supervisado**: Tenemos datos etiquetados

- Input ‚Üí Output conocido
- Ejemplo: Clasificar especies de ping√ºinos

**Aprendizaje No Supervisado**: NO tenemos etiquetas

- Solo tenemos los datos de entrada
- El algoritmo debe encontrar patrones por s√≠ mismo

---

## ¬øCu√°ndo Usar Aprendizaje No Supervisado?

- **Datos insuficientes etiquetados** para entrenar un modelo
- **Etiquetas de baja calidad** o inexactas
- **Etiquetar es muy costoso** en tiempo/recursos
- **No sabemos qu√© correlaciones** pueden existir
- **Queremos explorar** la estructura de los datos

---

## Tipos de Aprendizaje No Supervisado

:::: {.columns}

::: {.column width="50%"}
### Clustering

- Agrupar datos similares
- K-means
- Clustering espectral
- DBSCAN
:::

::: {.column width="50%"}
### Reducci√≥n de Dimensionalidad

- Simplificar representaci√≥n
- PCA
- t-SNE
- UMAP
:::

::::

---

# Parte I: Clustering

---

## ¬øQu√© es Clustering?

**Clustering** = Agrupar datos similares entre s√≠

- Puntos en el mismo cluster son **similares**
- Puntos en clusters diferentes son **diferentes**
- **Sin supervisi√≥n**: no sabemos las agrupaciones a priori

---

## Aplicaciones de Clustering

- **An√°lisis de tendencias** en datos
- **Segmentaci√≥n de clientes** en marketing
- **Compresi√≥n de im√°genes** (reducir colores)
- **Detecci√≥n de anomal√≠as**
- **Reconocimiento de patrones**

---

## K-means: Conceptos B√°sicos

**K-means** busca `k` centros que minimicen la distancia total

### Algoritmo:
1. Elegir `k` (n√∫mero de clusters)
2. Inicializar `k` centros aleatoriamente
3. Asignar cada punto al centro m√°s cercano
4. Recalcular centros como promedio de puntos asignados
5. Repetir pasos 3-4 hasta convergencia

---

## K-means: Hiperpar√°metro K

```{python}
# | echo: true
# K es un hiperpar√°metro que debemos especificar
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=4)  # ¬øPor qu√© 4?
```

**Problema**: ¬øC√≥mo elegir K?

**Estrategias**:

- Probar diferentes valores de K
- Usar m√©tricas de evaluaci√≥n (Silhouette Score)
- Conocimiento del dominio

---

## Implementaci√≥n con Scikit-Learn

```{python}
# | echo: true
import sklearn.cluster as skl_cluster
import sklearn.datasets as skl_datasets

# Generar datos de ejemplo
data, labels = skl_datasets.make_blobs(
    n_samples=400, 
    cluster_std=0.75, 
    centers=4, 
    random_state=1
)
```

---

## K-means: Ajuste del Modelo

```{python}
# | echo: true
# Inicializar K-means
kmeans = skl_cluster.KMeans(n_clusters=4)

# Entrenar el modelo
kmeans.fit(data)

# Predecir clusters
clusters = kmeans.predict(data)

# Obtener centros
centers = kmeans.cluster_centers_
```

---

## Evaluando Calidad: Silhouette Score

**Silhouette Score** mide qu√© tan bien ajustado est√° cada punto:

- **+1**: Perfectamente asignado a su cluster
- **0**: En la frontera entre clusters  
- **-1**: Probablemente mal asignado

```{python}
# | echo: true
from sklearn.metrics import silhouette_score

score = silhouette_score(data, clusters)
print(f"Silhouette Score: {score:.2f}")
```

---

## Ejercicio Pr√°ctico: Encontrar K √ìptimo

**Pregunta**: ¬øCu√°ntos clusters deber√≠amos usar?

**Tarea**: 
- Probar K = 2 a 10
- Calcular Silhouette Score para cada K
- ¬øQu√© K da el mejor score?

---

## Limitaciones de K-means

### Problemas comunes:

- **Requiere conocer K** de antemano
- **Solo fronteras lineales** entre clusters
- **Forma esf√©rica** de clusters
- **Sensible a outliers**
- **Clusters superpuestos** son problem√°ticos

---

## K-means: Clusters Conc√©ntricos

![Problema de clusters conc√©ntricos](https://via.placeholder.com/400x300/lightgray/black?text=Circles)

K-means **falla** cuando un cluster est√° dentro de otro

---

## Ventajas de K-means

- **Algoritmo simple** y f√°cil de entender
- **Computacionalmente eficiente**
- **Buena primera opci√≥n** para clustering
- **Escalable** a datasets grandes
- **Poca memoria** requerida

---

# Parte II: Clustering Espectral

---

## ¬øQu√© es Clustering Espectral?

**Clustering Espectral** supera las limitaciones lineales de K-means

### Idea clave:

- Trata clustering como **partici√≥n de grafos**
- Busca nodos con **distancias peque√±as** entre s√≠
- Usa **"kernel tricks"** para a√±adir dimensiones

---

## Clustering Espectral: Intuici√≥n

**Problema**: C√≠rculos conc√©ntricos

**Soluci√≥n**: A√±adir una dimensi√≥n Z que "separe" los c√≠rculos

- C√≠rculo interior ‚Üí Z alto
- C√≠rculo exterior ‚Üí Z bajo
- Ahora K-means funciona en 3D

---

## Implementaci√≥n de Clustering Espectral

```{python}
# | echo: true
import sklearn.datasets as skl_data
import sklearn.cluster as skl_cluster

# Crear c√≠rculos conc√©ntricos
circles, labels = skl_data.make_circles(
    n_samples=400, 
    noise=0.01, 
    random_state=0
)

# Clustering espectral
model = skl_cluster.SpectralClustering(
    n_clusters=2,
    affinity='nearest_neighbors',
    assign_labels='kmeans'
)

labels_spectral = model.fit_predict(circles)
```

---

## Comparaci√≥n: K-means vs Espectral

:::: {.columns}

::: {.column width="50%"}
### K-means
- ‚ùå Fronteras lineales
- ‚ùå Falla con c√≠rculos
- ‚úÖ Muy r√°pido
- ‚úÖ Poca memoria
:::

::: {.column width="50%"}
### Espectral  
- ‚úÖ Fronteras no-lineales
- ‚úÖ Maneja c√≠rculos
- ‚ùå M√°s lento
- ‚ùå M√°s memoria
:::

::::

---

## Costo Computacional

**Ejercicio de Timing**:

```{python}
# | echo: true
import time

start_time = time.time()
# ... algoritmo de clustering ...
end_time = time.time()

print(f"Tiempo: {end_time - start_time:.2f} segundos")
```

**Pregunta**: ¬øCu√°l es m√°s r√°pido para 4,000 puntos?

---

## Resultados de Performance

### 4,000 puntos:

- **K-means**: ~4 segundos
- **Espectral**: ~9 segundos (2.25x m√°s lento)

### 8,000 puntos:

- **K-means**: ~5.6 segundos  
- **Espectral**: ~24 segundos (4.28x m√°s lento)

**La diferencia aumenta exponencialmente** üìà

---

# Parte III: Reducci√≥n de Dimensionalidad

---

## ¬øPor Qu√© Reducir Dimensiones?

**Problema**: Datos de alta dimensionalidad

- Im√°genes 28√ó28 = 784 dimensiones
- Dif√≠cil de visualizar
- Clustering lento
- "Maldici√≥n de la dimensionalidad"

**Soluci√≥n**: Representar en menos dimensiones manteniendo informaci√≥n clave

---

## Dataset MNIST

**MNIST**: D√≠gitos escritos a mano (0-9)

- **70,000 im√°genes** originales (28√ó28 p√≠xeles)
- **Scikit-learn subset**: 1,797 im√°genes (8√ó8 p√≠xeles)
- **64 dimensiones** (una por p√≠xel)
- **Valores**: 0-15 (4 bits)

---

## Cargando el Dataset MNIST

```{python}
# | echo: true
from sklearn import datasets

# Cargar datos
features, labels = datasets.load_digits(
    return_X_y=True, 
    as_frame=True
)

print(features.shape)  # (1797, 64)
print(labels.shape)    # (1797,)
```

---

## Visualizando una Imagen

```{python}
# | echo: true
# Tomar la primera imagen
image_1D = features.iloc[0]
image_2D = np.array(image_1D).reshape(-1, 8)

# Mostrar imagen
plt.imshow(image_2D, cmap="gray_r")
plt.show()
```

**64 n√∫meros** ‚Üí **1 imagen reconocible**

---

## El Problema de 64 Dimensiones

- **Imposible visualizar** 64 dimensiones
- **Clustering directo** ser√≠a muy lento  
- **Relaciones complejas** entre p√≠xeles
- ¬øPodemos **simplificar** sin perder informaci√≥n?

---

# An√°lisis de Componentes Principales (PCA)

---

## ¬øQu√© es PCA?

**PCA** = Principal Component Analysis

**Objetivo**: Encontrar las direcciones de m√°xima varianza

- **Componente 1**: Direcci√≥n con m√°s variaci√≥n
- **Componente 2**: Segunda direcci√≥n con m√°s variaci√≥n  
- **Componente 3**: Tercera direcci√≥n...

---

## PCA: Implementaci√≥n

```{python}
# | echo: true
from sklearn import decomposition

# PCA con 2 componentes
pca = decomposition.PCA(n_components=2)
x_pca = pca.fit_transform(features)

print(x_pca.shape)  # (1797, 2)
```

**64 dimensiones** ‚Üí **2 dimensiones**

---

## Visualizando PCA

```{python}
# | echo: true
# Visualizar sin etiquetas
plt.scatter(x_pca[:, 0], x_pca[:, 1])
plt.title("PCA - Sin etiquetas")
plt.show()

# Visualizar con etiquetas verdaderas  
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=labels)
plt.colorbar()
plt.title("PCA - Con etiquetas")
plt.show()
```

---

## PCA + K-means

```{python}
# | echo: true
# Clustering en espacio reducido PCA
kmeans = skl_cluster.KMeans(n_clusters=10)
kmeans.fit(x_pca)
clusters_pca = kmeans.predict(x_pca)

# Visualizar clusters
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=clusters_pca)
plt.title("PCA + K-means")
plt.show()
```

---

## Resultados PCA

**Observaciones**:

- Algunos d√≠gitos se agrupan bien (0, 1, 4, 6)
- Otros se superponen mucho (5, 8)
- **PCA es lineal** ‚Üí limitaciones con patrones complejos
- **Preserva estructura global** ‚Üí puede perder detalles locales

---

# t-SNE: Embedding Estoc√°stico

---

## ¬øQu√© es t-SNE?

**t-SNE** = t-distributed Stochastic Neighbor Embedding

### Caracter√≠sticas:

- **No-lineal** (vs PCA lineal)  
- **Preserva vecindarios locales**
- **Manifold learning**
- **No-determin√≠stico** (resultados var√≠an)

---

## Concepto de Manifold

**Manifold** = Superficie de baja dimensi√≥n en espacio de alta dimensi√≥n

**Analog√≠a**: Papel arrugado

- Existe en espacio 3D (arrugado)
- Pero es inherentemente 2D (superficie del papel)
- **Manifold learning** = "desarrugar" los datos

---

## Intuici√≥n de t-SNE

**Objetivo**: Mantener vecinos cercanos juntos

**Analog√≠a**: Mover amigos del parque grande a jard√≠n peque√±o

- **Prioridad**: Amigos que hablan juntos permanezcan cerca
- **Sacrificio**: Distancias entre grupos pueden distorsionarse

---

## t-SNE: Implementaci√≥n

```{python}
# | echo: true
from sklearn import manifold

# t-SNE con inicializaci√≥n PCA
tsne = manifold.TSNE(
    n_components=2, 
    init='pca', 
    random_state=0
)
x_tsne = tsne.fit_transform(features)

plt.scatter(x_tsne[:, 0], x_tsne[:, 1])
plt.title("t-SNE - Sin etiquetas")
plt.show()
```

---

## t-SNE + K-means

```{python}
# | echo: true
# Clustering en espacio t-SNE
kmeans = skl_cluster.KMeans(n_clusters=10)
kmeans.fit(x_tsne)
clusters_tsne = kmeans.predict(x_tsne)

# Visualizar
plt.scatter(x_tsne[:, 0], x_tsne[:, 1], c=clusters_tsne)
plt.title("t-SNE + K-means")
plt.show()
```

---

## Comparaci√≥n: PCA vs t-SNE

:::: {.columns}

::: {.column width="50%"}
### PCA
- ‚úÖ R√°pido
- ‚úÖ Determin√≠stico  
- ‚úÖ Preserva estructura global
- ‚ùå Solo lineal
- ‚ùå Clusters superpuestos
:::

::: {.column width="50%"}
### t-SNE
- ‚ùå Lento
- ‚ùå No-determin√≠stico
- ‚ùå Distorsiona distancias globales
- ‚úÖ No-lineal
- ‚úÖ Excelente separaci√≥n local
:::

::::

---

## Interpretando t-SNE

**‚ö†Ô∏è Cuidado con la interpretaci√≥n**:

- **Distancias entre clusters** pueden ser enga√±osas
- **Clusters lejanos** en t-SNE pueden estar cerca en realidad
- **Clusters cercanos** en t-SNE pueden estar lejos en realidad
- **Usar para exploraci√≥n**, no para inferencias de distancia

---

## Ejercicio: 3 Dimensiones

**Pregunta**: ¬ø3D es mejor que 2D?

```{python}
# | echo: true
# PCA 3D
pca_3d = decomposition.PCA(n_components=3)
x_pca_3d = pca_3d.fit_transform(features)

# t-SNE 3D  
tsne_3d = manifold.TSNE(n_components=3, init='pca')
x_tsne_3d = tsne_3d.fit_transform(features)

# Plotting 3D...
```

---

## Hiperpar√°metros en t-SNE

**Par√°metros importantes**:

- `perplexity`: N√∫mero efectivo de vecinos cercanos
- `learning_rate`: Velocidad de optimizaci√≥n  
- `n_iter`: N√∫mero de iteraciones
- `init`: Inicializaci√≥n ('pca' vs 'random')

**Requiere experimentaci√≥n** para cada dataset

---

## Casos de Uso Pr√°cticos

### PCA mejor para:

- **An√°lisis exploratorio** r√°pido
- **Pre-procesamiento** para otros algoritmos
- **Compresi√≥n** de datos
- **Datasets grandes**

### t-SNE mejor para:

- **Visualizaci√≥n** de clusters complejos
- **Detecci√≥n de patrones** no-lineales  
- **An√°lisis cualitativo** detallado
- **Datasets medianos**

---

## Otros Algoritmos

**M√°s t√©cnicas disponibles**:

- **UMAP**: M√°s r√°pido que t-SNE, preserva m√°s estructura global
- **Isomap**: Para manifolds no-lineales  
- **LLE**: Locally Linear Embedding
- **Factor Analysis**: Similar a PCA pero probabil√≠stico

---

## Consideraciones Pr√°cticas

### Automatizaci√≥n:

- **¬øQu√© pasa si el algoritmo falla?**
- **¬øC√≥mo validar resultados?**
- **¬øSistemas cr√≠ticos vs exploraci√≥n?**

### Mejores pr√°cticas:

- **Siempre validar** con m√∫ltiples m√©todos
- **Entender limitaciones** de cada t√©cnica
- **Combinar** m√©todos complementarios

---

## Resumen: M√©todos No Supervisados

### Clustering:

- **K-means**: R√°pido, simple, fronteras lineales
- **Espectral**: Fronteras complejas, m√°s costoso

### Reducci√≥n de Dimensionalidad:

- **PCA**: Lineal, r√°pido, preserva estructura global  
- **t-SNE**: No-lineal, lento, excelente para visualizaci√≥n

---

## Workflow Recomendado

1. **Exploraci√≥n inicial** con PCA
2. **Visualizaci√≥n detallada** con t-SNE  
3. **Clustering** en espacio reducido
4. **Validaci√≥n** con m√©tricas apropiadas
5. **Interpretaci√≥n cuidadosa** de resultados

---

## Preguntas y Discusi√≥n

**¬øPreguntas sobre**:

- Clustering vs Clasificaci√≥n?
- Cu√°ndo usar cada t√©cnica?
- Casos espec√≠ficos de su trabajo?
- Implementaci√≥n pr√°ctica?

**¬°Gracias por su atenci√≥n!** üéØ
