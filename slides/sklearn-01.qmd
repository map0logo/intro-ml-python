---
title: Python para Aprendizaje AutomÃ¡tico 1
subtitle: IntroducciÃ³n y RegresiÃ³n
date: last-modified
author:
  - name: Francisco Palm
    orcid: 0000-0002-1293-0868
    email: fpalm@qu4nt.com
    affiliations: qu4nt, activistasxsl
format:
  clean-revealjs
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
lang: es
logo: images/qu4nt-logo.png
---

# IntroducciÃ³n al Machine Learning y MÃ©todos de RegresiÃ³n

------------------------------------------------------------------------

## Â¿QuÃ© aprenderemos hoy?

\

-   ğŸ¤– **Â¿QuÃ© es el machine learning?**
-   ğŸ“Š **Diferencias entre IA, ML y DL**
-   ğŸ” **Aplicaciones en la vida cotidiana**
-   ğŸ“ˆ **MÃ©todos supervisados: RegresiÃ³n**
-   ğŸ’» **PrÃ¡ctica con Scikit-Learn**

------------------------------------------------------------------------

## Â¿QuÃ© es Machine Learning?

\

> El machine learning es un conjunto de tÃ©cnicas que permiten a las computadoras usar datos para mejorar su rendimiento en una tarea especÃ­fica.

------------------------------------------------------------------------

## Â¿CÃ³mo aprenden las mÃ¡quinas?

\

-   ğŸ§  Similar a como los humanos aprenden de la experiencia
-   ğŸ“Š **"Dirigido por datos"** - usa estadÃ­sticas de los datos
-   ğŸ¯ Hace predicciones basadas en patrones encontrados

## Â¿Para quÃ© sirve el ML?

\

-   ğŸ” **Encontrar tendencias** en conjuntos de datos
-   ğŸ·ï¸ **Clasificar datos** en grupos o categorÃ­as\
-   ğŸ“Š **Hacer predicciones** basadas en datos
-   ğŸ® **"Aprender"** a interactuar con un entorno

## AI vs ML vs DL

\

![Diagrama de relaciones](images/AI_ML_DL_differences.png){fig-align="center"}

------------------------------------------------------------------------

## Inteligencia Artificial (AI)

\

-   ğŸŒŸ **TÃ©rmino mÃ¡s amplio** - diseÃ±ar dispositivs que se comporten de manera autÃ³noma garantizando un rendimiento
-   ğŸ“§ Desde filtros de spam hasta LLMs
-   ğŸ¯ El objetivo **sensato** es construir infraestructuras de IA basadas en datos de libre acceso y modelos de cÃ³digo abierto.

------------------------------------------------------------------------

## Machine Learning (ML)

\

-   ğŸ¯ **MÃ¡s especÃ­fico** - algoritmos que aprenden patrones de datos
-   ğŸ“š Necesita cientos/miles de ejemplos para aprender
-   ğŸ”’ Limitado a tareas especÃ­ficas o similares

------------------------------------------------------------------------

## Deep Learning (DL)

\

-   ğŸ§  **Redes neuronales** - basados en analogÃ¡s con el sistema nervioso
-   ğŸ“Š Puede resolver problemas diversos
-   âš¡ Puede requerir **enormes cantidades** de datos y poder computacional

# ML en Nuestra Vida Diaria


## Ejemplos Cotidianos

\

-   ğŸ¦ **Bancos**: DetecciÃ³n de fraudes en transacciones
-   ğŸ“§ **Email**: Filtros de spam inteligentes
-   ğŸ—ºï¸ **Apps de viaje**: EstimaciÃ³n de trÃ¡fico y rutas
-   ğŸ›’ **Retail**: Sistemas de recomendaciÃ³n


## MÃ¡s Ejemplos

\

-   ğŸ‘ï¸ **Reconocimiento**: ImÃ¡genes, objetos, patrones
-   ğŸš— **Coches autÃ³nomos**: DetecciÃ³n de objetos
-   ğŸ¬ **Streaming**: Contenido personalizado
-   ğŸ¤– **Robots**: InteracciÃ³n con el mundo

## ğŸ’¬ Momento de ReflexiÃ³n

\

**Comenta:**

1. Â¿DÃ³nde mÃ¡s has visto ML en uso?
2. Â¿QuÃ© tipo de datos usa ese sistema?
3. Â¿Tu interacciÃ³n ayuda a entrenar el sistema?
4. Â¿Has visto fallar algÃºn sistema?

# Limitaciones del ML

## ğŸ—‘ï¸ "Garbage In = Garbage Out"

\

-   Si los datos de entrada son basura...
-   ...la salida tambiÃ©n serÃ¡ basura
-   âš ï¸ **Ejemplo**: Buscar relaciÃ³n entre variables no relacionadas

## ğŸ“Š Sesgos en Datos de Entrenamiento

\

-   La calidad depende de la **amplitud** y **calidad** de los datos
-   Los sesgos en datos se reflejan en el modelo
-   âš ï¸ **Ejemplo**: Datos de transporte solo de Ã¡reas ricas

## ğŸ“ˆ Problemas de ExtrapolaciÃ³n

\

-   Solo podemos predecir dentro del **rango de entrenamiento**
-   Fuera de ese rango â†’ resultados no confiables
-   Algunos algoritmos son mejores para extrapolaciÃ³n

## ğŸ¯ Sobreajuste (Overfitting)

\

-   El modelo se "entrena demasiado"
-   Funciona mal con datos reales nuevos
-   âš ï¸ Como memorizar respuestas sin entender

## â“ Falta de Explicabilidad

\

-   Los modelos pueden dar respuestas incorrectas
-   La mayorÃ­a no puede explicar su lÃ³gica
-   Dificulta detectar y diagnosticar problemas


# IntroducciÃ³n a Scikit-Learn

## Â¿QuÃ© es Scikit-Learn?

\

-   ğŸ **Paquete de Python** para machine learning
-   ğŸŒ Construido por cientos de contribuidores
-   ğŸ­ Usado en industria y academia
-   ğŸ”§ API limpia y consistente

## Dependencias de Scikit-Learn

\

-   ğŸ“Š **NumPy**: ComputaciÃ³n numÃ©rica eficiente
-   ğŸ”¬ **SciPy**: ComputaciÃ³n cientÃ­fica
-   ğŸ“ DiseÃ±ado para datasets pequeÃ±os a medianos
-   ğŸ’» No requiere GPU para este curso

## Verificando la InstalaciÃ³n

\

```{python}
#| echo: true
import sklearn
print('scikit-learn:', sklearn.__version__)
```


# RepresentaciÃ³n de Datos en Scikit-Learn

## Estructura de Datos

\

Los algoritmos de ML esperan datos en **arreglos bidimensionales**:

```python
[n_samples, n_features]
```

-   **n_samples**: NÃºmero de muestras
-   **n_features**: NÃºmero de caracterÃ­sticas

## Matriz de CaracterÃ­sticas (X)

\

```python
# Variable X - caracterÃ­sticas/features
X = [[feature1, feature2, feature3],
     [feature1, feature2, feature3],
     ...]
```

-   Datos que usamos para entrenar
-   Generalmente valores reales

## Vector Objetivo (y)

\

```python
# Variable y - etiquetas/objetivos
y = [label1, label2, label3, ...]
```

-   Las "respuestas correctas"
-   Lo que queremos predecir


## Esquema Visual


![Entrada de Scikit-Learn](images/sklearn_input.png){fig-align="center"}

# Â¿QuÃ© cubriremos en este curso?

## Temas a tratar

\

-   ğŸ“š **Aprendizaje supervisado**
-   ğŸ“Š **Aprendizaje no supervisado**
-   ğŸ§  **IntroducciÃ³n a redes neuronales**
-   ğŸ”µ **Enfoque en tÃ©cnicas clÃ¡sicas** (marcadas en azul)

## Mapa del Machine Learning

![Resumen de ML](images/ML_summary.png){fig-align="center"}

# Aprendizaje Supervisado: RegresiÃ³n

## Â¿QuÃ© es el Aprendizaje Supervisado?

\

-   ğŸ‘¨â€ğŸ« Actuamos como "supervisor" o "maestro"
-   ğŸ“ Proporcionamos **datos etiquetados** con respuestas ejemplo
-   ğŸ¯ El algoritmo aprende de estos ejemplos

## Ejemplos de Aprendizaje Supervisado

\

-   ğŸ±ğŸ¶ **ClasificaciÃ³n**: Distinguir gatos de perros
-   ğŸ ğŸ’° **RegresiÃ³n**: Predecir precios de casas
-   ğŸ“Š Datos etiquetados = datos con respuestas conocidas

## ClasificaciÃ³n vs RegresiÃ³n

\

| ClasificaciÃ³n      | RegresiÃ³n           |
|--------------------|---------------------|
| ğŸ·ï¸ Datos discretos | ğŸ“ˆ Datos continuos  |
| Gato/Perro         | Precio: â‚¬250,000    |
| Spam/No spam       | Temperatura: 23.5Â°C |

------------------------------------------------------------------------

## Â¿QuÃ© es la RegresiÃ³n?

\

> TÃ©cnica estadÃ­stica que relaciona una **variable dependiente** (objetivo) con una o mÃ¡s **variables independientes** (caracterÃ­sticas).

## Objetivo de la RegresiÃ³n

\

-   ğŸ“Š **Describir la relaciÃ³n** entre variables
-   ğŸ¯ **Ajustar un modelo** matemÃ¡tico a los datos
-   ğŸ“ˆ **Hacer predicciones** con nuevos datos

------------------------------------------------------------------------

## Tipos de RegresiÃ³n

\

![Ejemplo de regresiones](images/regression_example.png)

-   **Lineal**: LÃ­nea recta
-   **Polinomial**: Curva

------------------------------------------------------------------------

## RegresiÃ³n Lineal

**EcuaciÃ³n matemÃ¡tica:**

$$     
y = mx + c
$$

-   **y**: variable objetivo
-   **x**: variable caracterÃ­stica\
-   **m**: pendiente
-   **c**: intersecciÃ³n con eje y

# PrÃ¡ctica con los Datos los PingÃ¼inos de Palmer

## Cargando los Datos

\

```{python}
#| echo: true
import seaborn as sns

dataset = sns.load_dataset("penguins")
print(dataset.shape)
dataset.head()
```

## Explorando el Dataset

\

-   ğŸ“Š **7 columnas** en total
-   4 continuas: `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`
-   3 categÃ³ricas: `species`, `island`, `sex`
-   âš ï¸ Algunos valores faltantes (`NaN`)

------------------------------------------------------------------------

## Limpieza de Datos

\

```{python}
#| echo: true
# Eliminar filas con valores faltantes
dataset.dropna(inplace=True)
dataset.head()
```

**Â¡Los datos faltantes son comunes en la vida real!**

------------------------------------------------------------------------

## Seleccionando Variables

\

-   **CaracterÃ­stica (X)**: `body_mass_g`
-   **Objetivo (y)**: `bill_depth_mm`
-   **Datos de entrenamiento**: Primeras 146 muestras

## Extrayendo Datos de Entrenamiento

\

```{python}
#| echo: true
import matplotlib.pyplot as plt

train_data = dataset[:146]  # primeras 146 filas

x_train = train_data["body_mass_g"]
y_train = train_data["bill_depth_mm"]
```

------------------------------------------------------------------------

## VisualizaciÃ³n Inicial

\

```{python}
#| echo: true
plt.scatter(x_train, y_train)
plt.xlabel("mass g")
plt.ylabel("depth mm")
plt.show()
```

------------------------------------------------------------------------

## Flujo de Trabajo en ML

\

1.  ğŸ¯ **Decidir** quÃ© modelo usar
2.  ğŸ”§ **Formatear** datos para el modelo
3.  ğŸ“š **Definir y entrenar** el modelo
4.  ğŸ”® **Hacer predicciones**
5.  âœ… **Verificar precisiÃ³n** y visualizar

------------------------------------------------------------------------

## Preparando los Datos

\

```{python}
#| echo: true
import numpy as np

# Scikit-Learn requiere arrays 2D
x_train = np.array(x_train).reshape(-1, 1)
y_train = np.array(y_train).reshape(-1, 1)

print(x_train.shape)  # (146, 1)
print(y_train.shape)  # (146, 1)
```

------------------------------------------------------------------------

## Creando el Modelo

\

```{python}
#| echo: true
from sklearn.linear_model import LinearRegression

# Definir el modelo
model = LinearRegression(fit_intercept=True)

# Entrenar el modelo
lin_regress = model.fit(x_train, y_train)
```

## Inspeccionando ParÃ¡metros

\

```{python}
#| echo: true
# Obtener parÃ¡metros del modelo entrenado
m = lin_regress.coef_        # pendiente
c = lin_regress.intercept_   # intersecciÃ³n

print("Coeficientes lineales:", m, c)
```

## Haciendo Predicciones

\

```{python}
#| echo: true
# Predecir usando el modelo entrenado
y_train_pred = lin_regress.predict(x_train)
```

**Â¡Realizamos predicciones sobre los mismos datos de entrenamiento para evaluar!**

------------------------------------------------------------------------

## Calculando Error RMSE

```{python}
#| echo: true
import math
from sklearn.metrics import mean_squared_error

# Calcular error cuadrÃ¡tico medio
error = math.sqrt(mean_squared_error(y_train, y_train_pred))
print("RMSE de entrenamiento =", error)
```

**RMSE = Root Mean Squared Error**

## Visualizando Resultados

```{python}
#| echo: true
plt.scatter(x_train, y_train, label="datos")
plt.plot(x_train, y_train_pred, "-", label="ajuste")
plt.plot(x_train, y_train_pred, "rx", label="predicciones")
plt.xlabel("body_mass_g")
plt.ylabel("bill_depth_mm")
plt.legend()
plt.show()
```

## Â¡Primer Modelo Completado! ğŸ‰

\

**Â¿QuÃ© hemos logrado?**

- âœ… Creado nuestro primer modelo de ML
- âœ… Podemos predecir `bill_depth_mm` para cualquier `body_mass_g`
- ğŸ“Š Tenemos una lÃ­nea de ajuste

------------------------------------------------------------------------

## Probando con MÃ¡s Datos

\

```{python}
#| echo: true
# Extraer datos restantes para prueba
test_data = dataset[146:]  # fila 147 hasta el final
x_test = test_data["body_mass_g"]
y_test = test_data["bill_depth_mm"]

# Formatear datos
x_test = np.array(x_test).reshape(-1, 1)
y_test = np.array(y_test).reshape(-1, 1)
```

## Predicciones en Datos de Prueba

\

```{python}
#| echo: true
# Predecir con datos nuevos
y_test_pred = lin_regress.predict(x_test)

# Calcular error en datos de prueba
error_test = math.sqrt(mean_squared_error(y_test, y_test_pred))
print("RMSE de prueba =", error_test)
```

## âš ï¸ Problema: Error MÃ¡s Alto

\

El RMSE en datos de prueba es **mucho mayor** que en entrenamiento.

**Â¿QuÃ© estÃ¡ pasando?**

## Visualizando el Problema

```{python}
#| echo: true
plt.scatter(x_train, y_train, label="entrenamiento")
plt.scatter(x_test, y_test, label="prueba")
plt.plot(x_train, y_train_pred, "-", label="ajuste")
plt.xlabel("body_mass_g")
plt.ylabel("bill_depth_mm")
plt.legend()
plt.show()
```

## ğŸš¨ Overfitting (Sobreajuste)

\

**Â¿QuÃ© observamos?**

- Dos grupos distintos de datos
- El modelo se ajusta bien a UN grupo
- Funciona mal en el otro grupo

## Â¿QuÃ© es el Overfitting?

\

-   ğŸ¯ El modelo aprende los datos **especÃ­ficos** de entrenamiento
-   ğŸ“š Como memorizar respuestas de un examen
-   âŒ No generaliza a datos nuevos
-   ğŸ” Especialmente comÃºn con datos limitados

## Mejorando la DivisiÃ³n de Datos

\

```{python}
#| echo: true
from sklearn.model_selection import train_test_split

x = dataset['body_mass_g']
y = dataset['bill_depth_mm']

# Formatear datos
x = np.array(x).reshape(-1, 1)
y = np.array(y).reshape(-1, 1)

# DivisiÃ³n aleatoria 80/20
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=0
)
```

## ğŸ‹ï¸ Ejercicio PrÃ¡ctico

\

**Reimplementa el modelo con la nueva divisiÃ³n:**

1.  âœ… Define el modelo
2.  ğŸ¯ Entrena con `.fit()`
3.  ğŸ”® ObtÃ©n predicciones con `.predict()`
4.  ğŸ“Š Calcula RMSE para entrenamiento y prueba
5.  ğŸ“ˆ Grafica datos y lÃ­nea de ajuste

## SoluciÃ³n del Ejercicio

\

```{python}
#| echo: true
from sklearn.linear_model import LinearRegression

# 1. Definir modelo
model = LinearRegression(fit_intercept=True)

# 2. Entrenar modelo
lin_regress = model.fit(x_train, y_train)

# 3. Predicciones
y_train_pred = lin_regress.predict(x_train)
y_test_pred = lin_regress.predict(x_test)
```

## Calculando Errores

\

```{python}
#| echo: true
# 4. Calcular RMSE
train_error = math.sqrt(mean_squared_error(y_train, y_train_pred))
test_error = math.sqrt(mean_squared_error(y_test, y_test_pred))

print("RMSE entrenamiento =", train_error)
print("RMSE prueba =", test_error)
```

## VisualizaciÃ³n Final

```{python}
#| echo: true
# 5. GrÃ¡fica
plt.scatter(x_train, y_train, label="entrenamiento")
plt.scatter(x_test, y_test, label="prueba")
plt.plot(x_train, y_train_pred, "-", label="ajuste")
plt.xlabel("body_mass_g")
plt.ylabel("bill_depth_mm")
plt.legend()
plt.show()
```

## InterpretaciÃ³n de Resultados

\

**Â¿Es este un buen modelo?**

- Â¿Es preciso?
- Â¿QuÃ© dice sobre la relaciÃ³n masa-profundidad del pico?
- Â¿DeberÃ­amos confiar en sus predicciones?


# RegresiÃ³n Polinomial

## Â¿Por quÃ© RegresiÃ³n Polinomial?

\

-   ğŸ“ˆ Los datos reales rara vez son perfectamente lineales
-   ğŸŒŠ Las funciones polinomiales pueden capturar curvas
-   ğŸ”¢ Forma matemÃ¡tica: `y = a + bx + cxÂ² + dxÂ³ + ... + mx^N`

## Grados Polinomiales

\

-   **Grado 1**: `y = a + bx` (lineal)
-   **Grado 2**: `y = a + bx + cxÂ²` (cuadrÃ¡tico)
-   **Grado 3**: `y = a + bx + cxÂ² + dxÂ³` (cÃºbico)
-   **Grado N**: Mayor complejidad


## Mismo Flujo de Trabajo

\

1.  âœ… Decidir modelo (polinomial)
2.  ğŸ”§ Formatear datos
3.  ğŸ“š Definir y entrenar modelo
4.  ğŸ”® Hacer predicciones\
5.  âœ… Verificar precisiÃ³n

## Preprocessing Polinomial

\

```{python}
#| echo: true
from sklearn.preprocessing import PolynomialFeatures

# Crear representaciÃ³n polinomial
poly_features = PolynomialFeatures(degree=2)
x_train_poly = poly_features.fit_transform(x_train)
x_test_poly = poly_features.transform(x_test)
x_train_poly[0:5]
```

## ğŸ§  Concepto Clave

\

**Convertimos un problema no-lineal en uno lineal:**

-   Transformamos las caracterÃ­sticas a representaciÃ³n polinomial
-   Usamos regresiÃ³n lineal sobre las caracterÃ­sticas transformadas
-   Â¡Problema no-lineal resuelto con tÃ©cnicas lineales!

------------------------------------------------------------------------

## Entrenando el Modelo Polinomial

\

```{python}
#| echo: true
# Usar LinearRegression con caracterÃ­sticas polinomiales
poly_regress = LinearRegression()
poly_regress.fit(x_train_poly, y_train)
```

**Â¡Es regresiÃ³n lineal sobre caracterÃ­sticas polinomiales!**

------------------------------------------------------------------------

## Predicciones y Errores

\

```{python}
#| echo: true
# Predicciones
y_train_pred = poly_regress.predict(x_train_poly)
y_test_pred = poly_regress.predict(x_test_poly)

# Errores
poly_train_error = math.sqrt(mean_squared_error(y_train_pred, y_train))
poly_test_error = math.sqrt(mean_squared_error(y_test_pred, y_test))

print("Error entrenamiento polinomial =", poly_train_error)
print("Error prueba polinomial =", poly_test_error)
```

## VisualizaciÃ³n Polinomial

\

```python
# Datos de entrenamiento y prueba
plt.scatter(x_train, y_train, label='Entrenamiento', color='blue', alpha=0.6)
plt.scatter(x_test, y_test, label='Prueba', color='red', alpha=0.6)

# LÃ­nea del modelo
x_range = np.linspace(min(x), max(x), 500).reshape(-1, 1)
y_range_pred = poly_regress.predict(poly_features.transform(x_range))
plt.plot(x_range, y_range_pred, label='Ajuste Polinomial', color='green', linewidth=2)

plt.xlabel("mass g")
plt.ylabel("depth mm")
plt.legend()
```

----

\

```{python}
# Datos de entrenamiento y prueba
plt.scatter(x_train, y_train, label='Entrenamiento', color='blue', alpha=0.6)
plt.scatter(x_test, y_test, label='Prueba', color='red', alpha=0.6)

# LÃ­nea del modelo
x_range = np.linspace(min(x), max(x), 500).reshape(-1, 1)
y_range_pred = poly_regress.predict(poly_features.transform(x_range))
plt.plot(x_range, y_range_pred, label='Ajuste Polinomial', color='green', linewidth=2)

plt.xlabel("mass g")
plt.ylabel("depth mm")
plt.legend()
```

## ğŸ‹ï¸ Ejercicio: Experimentar con Grados

\

**Cambia el parÃ¡metro `degree=` en `PolynomialFeatures`:**

-   Prueba `degree=3`, `degree=4`, `degree=5`
-   Â¿Puedes mejorar el RMSE?
-   Â¿QuÃ© observas al aumentar el grado?

# La Importancia del AnÃ¡lisis Exploratorio

## Â¿Por quÃ© MÃºltiples Clusters?

\

Cuando vemos **grupos distintos** en los datos, debemos preguntarnos:

**Â¿QuÃ© variable oculta causa estos clusters?**

## AnÃ¡lisis Exploratorio de Datos (EDA)

\

**Pasos crÃ­ticos antes de modelar:** 

- ğŸ” Investigar relaciones entre variables
- ğŸ“Š Verificar correlaciones 
- ğŸ“ˆ Graficar distribuciones
- âš ï¸ Buscar valores atÃ­picos
- ğŸ•³ï¸ Revisar valores faltantes

## Explorando con Pairplot

\

```python
# VisualizaciÃ³n por especies
sns.pairplot(dataset, 
             vars=["body_mass_g", "bill_depth_mm"], 
             hue="species", 
             diag_kind="kde", 
             markers=["o", "s", "D"])
plt.show()
```

----

\

```{python}
# VisualizaciÃ³n por especies
sns.pairplot(dataset, 
             vars=["body_mass_g", "bill_depth_mm"], 
             hue="species", 
             diag_kind="kde", 
             markers=["o", "s", "D"])
plt.show()
```

## Â¡RevelaciÃ³n! ğŸ‰

\

**Existen 3 conjuntos que corresponden a diferentes especies de pingÃ¼inos:**

- Cada especie tiene caracterÃ­sticas distintas
- La relaciÃ³n masa-profundidad varÃ­a por especie
- Necesitamos incluir `species` como variable predictora

## Modelo Multivariable

\

```{python}
#| echo: true
import pandas as pd

# Preparar datos con especies
dataset = dataset.dropna(subset=['body_mass_g', 'bill_depth_mm', 'species'])

# Definir predictores y objetivo
X = dataset[['body_mass_g', 'species']]
y = dataset['bill_depth_mm']
```

## One-Hot Encoding

\

```{python}
#| echo: true
# Convertir variables categÃ³ricas a numÃ©ricas
X = pd.get_dummies(X, columns=['species'], drop_first=True)

# drop_first=True evita multicolinealidad

X.head()
```

**Â¿Por quÃ©?** Los algoritmos de ML necesitan datos numÃ©ricos.

## Entrenamiento Final

\

```{python}
#| echo: true
# DivisiÃ³n de datos
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Ajuste del modelo
model = LinearRegression()
model.fit(x_train, y_train)

# PredicciÃ³n y evaluaciÃ³n

y_pred_train = model.predict(x_train)
rmse_train = mean_squared_error(y_train, y_pred_train)
print(f"RMSE de entrenamiento: {rmse_train:.2f}")
y_pred = model.predict(x_test)
rmse = mean_squared_error(y_test, y_pred)
print(f"RMSE con especies: {rmse:.2f}")
```

## Interpretando Coeficientes

\

```{python}
#| echo: true
# Ver coeficientes aprendidos
coefficients = pd.Series(model.coef_, index=X.columns)
print("\nCoeficientes del modelo:")
print(coefficients)
```

# Resumen y Conclusiones

## Â¿QuÃ© Hemos Aprendido?

\

-   ğŸ¤– **Conceptos bÃ¡sicos** de Machine Learning
-   ğŸ“Š **Diferencias** entre AI, ML, y DL
-   ğŸ”§ **Uso de Scikit-Learn** para regresiÃ³n
-   âš ï¸ **Limitaciones** y problemas comunes
-   ğŸ” **Importancia** del anÃ¡lisis exploratorio

## Lecciones Clave

\

1.  ğŸ—‘ï¸ **Calidad de datos** = calidad del modelo
2.  ğŸ¯ **Overfitting** es un problema real
3.  ğŸ“Š **DivisiÃ³n aleatoria** de datos es crucial
4.  ğŸ” **EDA primero**, modelado despuÃ©s
5.  ğŸ§© **Variables adicionales** pueden resolver problemas

## Mejores PrÃ¡cticas

\

-   âœ… Siempre explorar datos primero
-   âœ… Dividir datos aleatoriamente
-   âœ… Evaluar en datos no vistos
-   âœ… Considerar variables adicionales
-   âœ… Interpretar coeficientes del modelo

## PrÃ³ximos Pasos

\

-   ğŸ“š **ClasificaciÃ³n** (prÃ³xima lecciÃ³n)
-   ğŸ§© **Aprendizaje no supervisado**
-   ğŸ§  **Redes neuronales**
-   ğŸ”§ **MÃ¡s tÃ©cnicas de preprocessing**
-   ğŸ“Š **ValidaciÃ³n cruzada**

## Â¿Preguntas?

\

**Â¡Hora de practicar y experimentar!**

ğŸ¤” Â¿QuÃ© otros datasets te gustarÃ­a explorar?

ğŸ”¬ Â¿QuÃ© problemas reales podrÃ­as resolver con regresiÃ³n?

## Recursos Adicionales

\

-   ğŸ“– [Scikit-Learn Documentation](https://scikit-learn.org/)
-   ğŸ“š Python Data Science Handbook
-   ğŸŒ Kaggle Learn (cursos gratuitos)
-   ğŸ“Š Dataset repositories: UCI, Kaggle, Seaborn

**Â¡Gracias por su atenciÃ³n!**

ğŸ‰ **Â¡Felicidades por completar su primer modelo de Machine Learning!**